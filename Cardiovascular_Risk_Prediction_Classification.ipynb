{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "K5QZ13OEpz2H",
        "448CDAPjqfQr",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jamal-1018/Cardiovascular_Risk_Prediction_Classification/blob/main/Cardiovascular_Risk_Prediction_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Cardiovascular Risk Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - **Classification**\n",
        "##### **Contribution**    - **Individual**\n",
        "##### **Team Member**     - **Mohammad Jamaluddin**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#for plotting charts\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# for statistical tests\n",
        "from scipy.stats import chi2_contingency,chi2\n",
        "\n",
        "# for imputing the missing values\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# For multicolinearity of VIF technique\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# For train test of the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "# For scaling the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DteJ9jolrZf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "path = '/content/drive/MyDrive/ALMABETTER CAPSTONE PROJECTS/CLASSIFICATION_cardiovascular_Risk_prediction/data_cardiovascular_risk.csv'\n",
        "\n",
        "DATA = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "DATA.head(5)  #First 5 rows"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data last 5 rows\n",
        "DATA.tail(5)"
      ],
      "metadata": {
        "id": "yH3xQIiQsCBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f'The number of rows in the data is : {DATA.shape[0]}')\n",
        "print(f'The number of columns in the data is : {DATA.shape[1]}')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "DATA.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating copy of dataset\n",
        "df = DATA.copy()"
      ],
      "metadata": {
        "id": "jTHxG0S96BX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull())"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dataset have 3390 rows and 17 columns and the TenYearCHd is dependent variable.\n",
        "- It does not have any duplicate rows.\n",
        "\n",
        "- It contains missing values in education,CigsPerDay,BPMeds.totChol,BMI,glucose columns."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include = 'all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demographic:\n",
        "\n",
        "- id : id of the patitent\n",
        "\n",
        "- age : age of the patient\n",
        "\n",
        "- education : education of the patient\n",
        "\n",
        "- sex : sex of the patient (M/F)\n",
        "\n",
        "Behaviour:\n",
        "\n",
        "- is_smoking : If the patient has the habit of smoking (YES/NO)\n",
        "\n",
        "- cigsPerDay : The average number of ciagrettes that the patient smokes `continuous`\n",
        "\n",
        "Medical History:\n",
        "\n",
        "- BPMeds : Whether or not the patient takes the Blood Pressure medication or not (1/0)\n",
        "\n",
        "- prevalentStroke : Whether or not  the patient has any history of stroke (1/0)\n",
        "\n",
        "- prevalentHyp : Whether or not the patient has any history of hypertension (1/0)\n",
        "\n",
        "- diabetes : WHether or not the patient has diabetes (1/0)\n",
        "\n",
        "Medical (Current) :\n",
        "\n",
        "- totChol : Total Cholestrol level `Continuous`\n",
        "\n",
        "- sysBP : siastol levels in blood pressure `Continuous`\n",
        "\n",
        "- diaBP : Diastol levels in blood pressure `continuous`\n",
        "\n",
        "- BMI : Body Mass Index of the patient `Continuous`\n",
        "\n",
        "- heartrate : Heart Rate of the patient `Continuous`\n",
        "\n",
        "- glucose : Glucose levels of the patient `continuous`\n",
        "\n",
        "Target variable:\n",
        "\n",
        "- TenYearCHD : 10 year Coronary Heart Disease (1: Yes, 0: No) `dependent varaible`"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(f'the number of unique values in {i} is {df[i].nunique()}')"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping id column\n",
        "df.drop('id', axis = 1, inplace = True )"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# renaming the columns\n",
        "df.rename(columns = {'cigsPerDay': 'cigs_per_day','BPMeds':'bp_meds','prevalentStroke': 'prevalent_stroke',\n",
        "           'prevalentHyp' : 'prevalent_hyp','totChol' : 'total_chol', 'sysBP' : 'sys_bp',\n",
        "                     'diaBp' : 'dia_bp','BMI': 'bmi','heartRate': 'heart_rate','TenYearCHD' : 'ten_year_chd'},inplace = True)"
      ],
      "metadata": {
        "id": "ffEfZTbv9Tj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After renaming the columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "QAHfjZmD_GsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categorizing the variables\n",
        "dep_var = ['ten_year_chd']\n",
        "dep_var"
      ],
      "metadata": {
        "id": "s2PKBvTc-yhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# continuous variables\n",
        "cont_var = ['age','cigs_per_day','total_chol','sys_bp','diaBP','bmi','heart_rate','glucose']\n",
        "cont_var"
      ],
      "metadata": {
        "id": "_Cnf9OUp_nkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical variables\n",
        "cat_var = [i for i in df.columns if i not in cont_var]\n",
        "cat_var.remove('ten_year_chd')\n",
        "cat_var"
      ],
      "metadata": {
        "id": "QsJQ1n-z_nnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Column names have been renamed for the ease of use\n",
        "\n",
        "- Categorized the continuous, dependent and cateforical variabes for futher analysis"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 Visualization on dependent variable `ten_year_chd`"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting countplot on dependent variables\n",
        "plt.figure(figsize = (8,8))\n",
        "ax = sns.countplot(data = df, x = df['ten_year_chd'])\n",
        "total = len(df.ten_year_chd)\n",
        "for p in ax.patches:\n",
        "  ax.annotate(f'{100* p.get_height()/total:.1f}%',(p.get_x() + p.get_width()/2,\n",
        "              p.get_height()), ha ='center', va = 'center', fontsize = 12)\n",
        "\n",
        "plt.xlabel('CHD Risk (0: No CHD, 1: CHD)')\n",
        "plt.ylabel('Count')\n",
        "plt.title('CHD(%)')\n",
        "plt.show()\n",
        "\n",
        "print(f'The No risk of CHD is: {df.ten_year_chd.value_counts()[0]}')\n",
        "print(f'The Risk of CHD is: {df.ten_year_chd.value_counts()[1]}')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A countplot is a type of bar chart used in data visualization to display the count or frequency of categorical data. It's a straightforward and effective way to represent the distribution of categorical variables.We used this chart to represent the dependent variables and its distribution."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, we have the following insights:\n",
        "- The values representing 'No risk of CHD' account for the majority at 84.9%, with a total of 2,879 occurrences i.e negative.\n",
        "- Conversely, the values representing 'Risk of CHD' are in the minority at 15.1%, totaling 511 occurrences i.e positive."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classification of patients into positive and negative categories based on their health risk is crucial for building predictive models. This classification provides valuable insights for healthcare industries, guiding the development of effective strategies and interventions. However, it's essential to note that failing to address a high risk of CHD in a patient could have a negative impact on both the patient's health and the healthcare businesses as a whole."
      ],
      "metadata": {
        "id": "5x6oNbhS20hK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2  Visualization of distribution and box plots on continuous variables"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting of distribution ad boxplots\n",
        "for col in df.describe().columns:\n",
        "  fig,axes = plt.subplots(nrows = 1, ncols =2 , figsize = ( 16,5))\n",
        "  sns.histplot(df[col], ax = axes[0],kde = True)\n",
        "  sns.boxplot(df[col], ax = axes[1],showmeans = True, color = 'orange')\n",
        "  fig.suptitle(f'The distribution of {col}')\n",
        "  plt.show()\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "kjAvY2QNCO3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histplot is a type of chart that displays the distribution of a dataset. It is a graphical representation of the data that shows how often each value or group of values occurs. Histplots are useful for understanding the distribution of a dataset and identifying patterns or trends in the data. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data.\n",
        "\n",
        "Thus, I used the histogram plot to analysis the variable distributions over the whole dataset whether it's symmetric or not.\n",
        "\n",
        "A boxplot is used to summarize the key statistical characteristics of a dataset, including the median, quartiles, and range, in a single plot. Boxplots are useful for identifying the presence of outliers in a dataset, comparing the distribution of multiple datasets, and understanding the dispersion of the data. They are often used in statistical analysis and data visualization.\n",
        "\n",
        "Thus, for each numerical varibale in the given dataset, I used box plot to analyse the outliers and interquartile range including mean, median, maximum and minimum value."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, we have observed that the majority of the variables in the dataset exhibit a normal distribution. However, some variables appear to be skewed, indicating a departure from normality. Additionally, there are outliers present in certain variables as well"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These insights are important for understanding the characteristics of the data and can inform our data preprocessing and modeling decisions."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 Visulaization of categorical variables"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['ten_year_chd'].value_counts()"
      ],
      "metadata": {
        "id": "zWUbfGO8jJ4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting of counter plot on categorical columns\n",
        "for i in cat_var:\n",
        "  plt.figure(figsize = (10,5))\n",
        "  p = sns.countplot(x = df[i], data =df)\n",
        "  total_len = len(df[i])\n",
        "  for i in p.patches:\n",
        "    p.annotate(f'{(100 * i.get_height()/total_len):.2f}%',((i.get_x()+i.get_width()),(i.get_height())),\n",
        "               ha = 'center', va = 'center', fontsize = 10)\n",
        "  plt.show()\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A countplot is a type of bar chart used in data visualization to display the count or frequency of categorical data. It's a straightforward and effective way to represent the distribution of categorical variables.Here we used this char on all categorical columns to represent the distribution of the data."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, the following insights are observed:\n",
        "\n",
        "- The 'education' variable is most prominently represented by category 1, accounting for 41.03% of the data, while the other categories each have less than 29% representation.\n",
        "\n",
        "- The 'sex' variable exhibits a well-balanced distribution, with 43.2% for males and 56.7% for females.\n",
        "\n",
        "- The 'smoker' variable also demonstrates a balanced distribution, with 49.76% being smokers and 50.24% non-smokers.\n",
        "\n",
        "- On the other hand, variables such as 'bp_meds,' 'prevalent_stroke,' 'prevalent_hypertension,' and 'diabetes' show an imbalanced distribution of data."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights have the potential to create a positive business impact.However, the extent of the impact depends on how these insights are leveraged and applied to meet specific business objectives. Understanding the distribution of variables is a crucial step in data-driven decision-making and can lead to more informed and effective strategies."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Visualization of categorical variables and distribution"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting a categorical plot on all categorical variables\n",
        "for i in cont_var:\n",
        "  plt.figure(figsize = (10,8))\n",
        "  sns.catplot(x = dep_var[0], y = i, data = df, kind = 'violin')\n",
        "  plt.title(dep_var[0]+' vs '+ i)\n",
        "  plt.show()\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The violin chart is a type of data visualization that combines aspects of a box plot and a kernel density plot. It is used to display the distribution and density of data across different categories or groups. Some features of the violin chart include:\n",
        "\n",
        "- Shape and width: The shape of the violin represents the data distribution, typically displaying a mirrored, symmetrical shape. The width of the violin at different points indicates the density of data.\n",
        "\n",
        "- Quartiles and median: The central \"box\" in the violin chart represents the interquartile range (IQR) and contains the median value. This provides insights into the spread and central tendency of the data.\n",
        "\n",
        "Grouping and comparison: Violin charts can be grouped or arranged side by side to compare distributions across different categories or groups. This allows for visual comparisons of data distribution shapes, spreads, and densities.\n",
        "\n",
        "Here, we used this chart on all continuous variables with dependent variable to observe the distribution and central tendency of the data."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the insights:\n",
        "\n",
        "-  Instances of a positive risk of CHD are higher for individuals with age values greater than 50 when compared to those with a negative risk of CHD.\n",
        "\n",
        "- The density of 'cigsperday' values is higher in occurrences with a negative risk of CHD as opposed to those with a positive risk of CHD.\n",
        "\n",
        "- The density of 'glucose' values is higher in occurrences with a negative risk of CHD as opposed to those with a positive risk of CHD."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the violin chart can potentially create a positive business impact by providing valuable information for decision-making. Understanding the distribution and density of data across different categories can help businesses identify patterns, trends, and potential areas of focus. For example, the insight that positive cases of ten-year CHD are higher in older people suggests the need for targeted preventive measures or specialized treatments for this demographic. Similarly, the insight regarding the relationship between smoking and negative cases of CHD can inform smoking cessation programs or campaigns to reduce the risk of CHD.\n",
        "\n",
        "While the insights gained from the chart can be valuable, it's important to note that the impact on business growth would depend on various factors. The actual business impact would require further analysis and strategic implementation of these insights. Additionally, without specific business context and objectives, it is challenging to determine if there are any insights that would directly lead to negative growth. However, using the insights to better understand the distribution of health conditions and risk factors can potentially help businesses in the healthcare industry develop more effective strategies and interventions to improve patient outcomes and drive positive growth."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 Visualization of categorical variables with stacked bar charts"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting of stacked bar charts on categorical variables to observe percenatage of occurence of the dependent variables\n",
        "for col in cat_var:\n",
        "  grouped_data = df.groupby(col)['ten_year_chd'].value_counts(normalize  =True).unstack('ten_year_chd') * 100\n",
        "  plt.figure(figsize = (10,8))\n",
        "  grouped_data.plot.barh(stacked = True)\n",
        "\n",
        "# plotting the graph to show percentages\n",
        "  for ix, row in grouped_data.reset_index(drop = True).iterrows():\n",
        "    cummulative = 0\n",
        "    for element in row:\n",
        "      if element > 0.1:\n",
        "        plt.text(cummulative + element /2 ,\n",
        "                 ix,\n",
        "                 f\"{int(element)} %\",\n",
        "                 ha = 'center',\n",
        "                 va = 'center')\n",
        "      cummulative += element\n",
        "  plt.show()\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A horizontal stacked bar chart is a type of data visualization designed to illustrate the composition or proportion of various categories within a whole.The chart presents multiple categories or groups stacked horizontally, enabling straightforward visual comparisons of their relative proportions within the total. Each bar represents the whole, with its segments depicting the different categories or components."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, these are the below insights:\n",
        "- The percentage of CHD is slightly higher in the 'education' variable's class 1.0 when compared to other classes of the variable.\n",
        "\n",
        "- For the 'sex' and 'is_smoker' variables, it's noticeable that the CHD risk is slightly greater in the 'Male' class and 'Smoker: Yes' class.\n",
        "\n",
        "- Among the other variables ('bp_meds,' 'prevalent_stroke,' 'prevalent_hypertension,' and 'diabetes'), the percentage of CHD risk is significantly higher for the 'Yes' classes when compared to the 'No' classes, respectively."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the distribution and composition of different categories in relation to the occurrence of CHD. These insights can guide businesses in developing targeted strategies and interventions to address specific demographic groups or risk factors. For example, businesses can design preventive measures, educational campaigns, or tailored treatments to reduce the occurrence of CHD in high-risk categories. By leveraging these insights, businesses in the healthcare industry can improve patient outcomes, enhance customer satisfaction, and drive positive growth.\n",
        "\n",
        "There are no specific insights from the chart that directly lead to negative growth. However, it's important to consider the overall prevalence of CHD and the effectiveness of interventions. If the prevalence of CHD remains high across all categories and the implemented strategies fail to yield desired outcomes, it could potentially result in negative growth due to increased healthcare costs, decreased patient satisfaction, or reputational issues. Therefore, the business impact ultimately depends on the successful implementation of strategies based on the insights gained from the chart."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 Correlation Heatmap"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting of heatmap\n",
        "corr = df.corr()\n",
        "mask = np.zeros_like(corr)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "plt.figure(figsize = (18,9))\n",
        "sns.heatmap(corr,mask = mask, annot =True, vmin = -1,vmax = 1,cmap = \"coolwarm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8ollOuc9bcwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting bar chart to find the correlation of every independent variable with dependent varable\n",
        "df.drop('ten_year_chd',axis = 1).corrwith(df['ten_year_chd']).plot(kind = 'bar',grid = True,figsize = (8,3))\n",
        "plt.ylim(-1.0,1.0)"
      ],
      "metadata": {
        "id": "bqNCi7xTzGxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation coefficient is a measure of the strength and direction of a linear relationship between two variables. A correlation matrix is used to summarize the relationships among a set of variables and is an important tool for data exploration and for selecting which variables to include in a model.\n",
        "\n",
        "The range of correlation is [-1,1].\n",
        "\n",
        "- If the value is towards 1, then there is a positive correlation which means if one variable value increases then the other value also increases.\n",
        "- If the value is towards -1 then there is negative correlation which means if one value increase then the other value decreases.\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, we have used correlation heatmap."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, below insights are:\n",
        "- The 'sys_bp' and 'diaBP' are strongly positively correlated with a coefficient of 0.78.\n",
        "- 'Diabetes' and 'glucose' exhibit a slightly positive correlation with a coefficient of 0.68.\n",
        "- 'sysBP' and 'diaBP' display slightly positive correlations with 'prevalent_hypertension' with coefficients of 0.7 and 0.61, respectively."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 7 -  Plotting of pairplot"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting of pairplot\n",
        "sns.pairplot(df, hue = 'ten_year_chd')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pairplot, also known as a scatterplot matrix, is a visualization that allows you to visualize the relationships between all pairs of variables in a dataset. It is a useful tool for data exploration because it allows you to quickly see how all of the variables in a dataset are related to one another.\n",
        "\n",
        "Thus, we used pair plot to analyse the patterns of data and relationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, below insights are:\n",
        "- The cigs_per_day and education have skewed distributions,so we will perform futher analysis and transformations."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check categorical variables are related with dependent varaible\n",
        "- Null hypothesis (H0) : categorical and dependent variables are independent\n",
        "\n",
        "- Alternate hypothesis (H1): categorical and dependent variables are not independent"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value(Chi2)\n",
        "def chi_metric(dataframe):\n",
        "\n",
        "  p_values = []\n",
        "  chi_squared_values = []\n",
        "  for column in dataframe.columns:\n",
        "    cross_tab = pd.crosstab(df[column], df['ten_year_chd'])\n",
        "    chi2, p, dof, expected = chi2_contingency(cross_tab)  #Using chi2 contingency table\n",
        "    p_values.append(p)\n",
        "    chi_squared_values.append(chi2)\n",
        "\n",
        "    print(f'Analyzing variable: {column}')\n",
        "    print(f'Chi-squared: {chi2}')\n",
        "    print(f'p-value: {p}')\n",
        "    print('-' * 20)\n",
        "  print(p_values)\n",
        "\n",
        "# Plotting the p_value of the category varaibles\n",
        "  pd.Series(p_values, index = dataframe.columns).plot.barh()\n",
        "  plt.xscale('log')\n",
        "  plt.title('P_value for categorical variables')\n",
        "  plt.xlabel('P_value')\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat = df[cat_var]\n",
        "chi_metric(df_cat)"
      ],
      "metadata": {
        "id": "wazGx99yckle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p_value for all the variables are lesser than the p_value, so we reject null hypothesis."
      ],
      "metadata": {
        "id": "Ji4G-nguf-lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the hypothesis that the categorical and dependent variables are independent or not,we performed a chi-squared test of independence. This statistical test allowed to determine the relationship. By calculating the chi-squared statistic and p-value, we can make a statistical inference about the relationship between the variables in the dataset."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We performed the chi-squared test of independence to test the hypothesis that the categorical variables and dependent variable are not independent to each other because it is an appropriate statistical test for determining if there is a significant association between two categorical variables. In this case, both variabels are categorical variables, so the chi-squared test is a suitable choice.\n",
        "\n",
        "The chi-squared test works by comparing the observed frequency distribution of the data in a contingency table to the expected frequency distribution under the assumption that the null hypothesis is true. If there is a significant difference between the observed and expected frequencies, it suggests that there is a relationship between the two variables.\n",
        "\n",
        "Overall, we used the chi-squared test of independence because it is a widely used and well-established statistical test for analyzing the relationship between two categorical variables. It allowed us to make a statistical inference about the relationship between categorical variables and dependent variables."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a copy of my dataset 2\n",
        "df2 = df.copy()\n"
      ],
      "metadata": {
        "id": "rhrEgM1MG__3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# variables which have missing/Nan values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Handling missing values in categorical varibles"
      ],
      "metadata": {
        "id": "E6WlncjRtIJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replacing the null values with mode values of the categorical variables\n",
        "df['bp_meds'] = df['bp_meds'].fillna(df['bp_meds'].mode()[0])\n",
        "df['education'] = df['education'].fillna(df['education'].mode()[0])"
      ],
      "metadata": {
        "id": "qFKG8oZ7tR5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Handling the missing values in continous variables of `cig_per_day`"
      ],
      "metadata": {
        "id": "NRU156O2ukIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mean and median of the cigs per day\n",
        "df['cigs_per_day'].mean().round(0), df['cigs_per_day'].median()"
      ],
      "metadata": {
        "id": "U_dN_rGAun7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['cigs_per_day'].isnull()][['is_smoking','cigs_per_day']]"
      ],
      "metadata": {
        "id": "W_emrw-wvK-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can observe that for every NaN value, the is_smoking value is YES. which assures that cigs per day variable is not 0."
      ],
      "metadata": {
        "id": "rNiEvdXAvwHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the mean and median of cigs per day excluding non smokers\n",
        "df[df['is_smoking'] == 'YES']['cigs_per_day'].mean(),df[df['is_smoking'] == 'YES']['cigs_per_day'].median()"
      ],
      "metadata": {
        "id": "1VzkP2T1wGis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputting with the median values in the NaN values\n",
        "df['cigs_per_day'] = df['cigs_per_day'].fillna(df[df['is_smoking'] == 'YES']['cigs_per_day'].median())"
      ],
      "metadata": {
        "id": "bGX9srqOwoVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for missing entries like where patient is_smoking is yes & cigs per day is 0\n",
        "df[(df['is_smoking'] == 'YES') & (df['cigs_per_day'] == 0)]"
      ],
      "metadata": {
        "id": "WX-LqVHVwoas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Handling the total cholestrol,bmi,heartrate variable"
      ],
      "metadata": {
        "id": "4wD5p2fAxpLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mean,median of total cholestrol variable\n",
        "df['total_chol'].mean(),df['total_chol'].median()"
      ],
      "metadata": {
        "id": "q5fU27PhxxTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputing with the median values in the NaN values\n",
        "df['total_chol'] = df['total_chol'].fillna(df['total_chol'].median())"
      ],
      "metadata": {
        "id": "1Fi-Q103yY6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean,median of bmi variable\n",
        "df['bmi'].mean(), df['bmi'].median()"
      ],
      "metadata": {
        "id": "jJ962KXlwod0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputting with the median variable\n",
        "df['bmi'] = df['bmi'].fillna(df['bmi'].median())"
      ],
      "metadata": {
        "id": "F573vja6y_pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean, median of heart rate variables\n",
        "df['heart_rate'].mean(), df['heart_rate'].median()"
      ],
      "metadata": {
        "id": "Wtbajdaoy_s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing with the median values of the variable\n",
        "df['heart_rate'] = df['heart_rate'].fillna(df['heart_rate'].median())"
      ],
      "metadata": {
        "id": "oWHB2sWu0fTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Handling glucose variable"
      ],
      "metadata": {
        "id": "crLLBuee0zMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['glucose'].mean().round(0), df['glucose'].median()"
      ],
      "metadata": {
        "id": "H-xHws6G0y2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The data spread being skewed positively and values falling outside the typical range, a more careful approach is needed, especially for the glucose column with 304 missing data points. Simply using the mean or median for imputation may introduce significant inaccuracies.\n",
        "\n",
        "- One effective solution is to employ the KNN imputer method. This method considers the relationships between data points and is more suitable for imputing missing values, ensuring that the imputed values align better with the overall data distribution."
      ],
      "metadata": {
        "id": "ZU5eyh9u3Vq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying KNNImputer to the glucose variable\n",
        "imputer = KNNImputer(n_neighbors = 5)\n",
        "imputed_column = imputer.fit_transform(df[['glucose']])\n",
        "df['glucose'] = imputed_column"
      ],
      "metadata": {
        "id": "hYzFNn7X2zSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After imputing, mean and median of the column\n",
        "df['glucose'].mean(), df['glucose'].median()"
      ],
      "metadata": {
        "id": "z20z8Yk35AFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for the missing/null values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "WB9rW4y_5yBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our data preprocessing, we've employed multiple imputation techniques to address missing values in the dataset. Here's how we've applied these techniques:\n",
        "\n",
        "1. **Median Imputer for Skewed Continuous Variables:** For continuous variables that exhibited skewness, we opted for the median imputation technique. The median is a robust measure of central tendency that is not influenced by outliers, making it a reliable choice for filling missing values in skewed data.\n",
        "\n",
        "2. **Mode Imputer for Categorical Variables:** When dealing with categorical variables, we utilized the mode imputation technique. The mode represents the most common and frequent value within a category, making it a suitable estimate for handling missing values in categorical data.\n",
        "\n",
        "3. **KNN Imputer for Continuous Variables:** We used KNN Imputer technique that imputes missing values using the k-nearest neighbors algorithm. It replaces missing values by taking into account the values of the k-nearest neighbors of the missing data point. This method can be particularly useful when there is a complex relationship between features, and simple imputation methods like mean or median are not appropriate.\n",
        "\n",
        "By selecting appropriate imputation techniques based on the data type and characteristics of the variables, we aimed to ensure the robustness and accuracy of the imputed values while preparing the dataset for analysis.\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers with IQR(Inter Quartile Range) technique\n",
        "for col in cont_var:\n",
        "  q1, median, q3 = df[col].quantile([0.25,0.50,0.75])\n",
        "  iqr = q3 - q1\n",
        "  upper_limit = q3 + 1.5 * iqr\n",
        "  lower_limit = q1 - 1.5 * iqr\n",
        "\n",
        "  # Replacing with the outliers\n",
        "  df[col] = np.where(df[col] > upper_limit, upper_limit, df[col])\n",
        "  df[col] = np.where(df[col] < lower_limit, lower_limit, df[col])\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Applying the IQR function on continuous variables of the data\n",
        "df[cont_var]"
      ],
      "metadata": {
        "id": "wBMuudeh-jvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have applied the Interquartile Range (IQR) method to identify and eliminate outliers present in the continuous columns of the dataset. This method was chosen due to its robustness in outlier detection, as it remains unaffected by extreme values. The IQR is computed as the difference between the 75th and 25th percentiles of the data, and any data point falling below the 25th percentile minus 1.5 times the IQR or exceeding the 75th percentile plus 1.5 times the IQR is categorized as an outlier. By utilizing the IQR method, We ensured a consistent and objective approach to identify and remove outliers from the dataset."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Encoding categorical variables\n",
        "df['sex'] = np.where(df['sex'] == 'M',1,0)\n",
        "df['is_smoking'] = np.where(df['is_smoking'] == 'YES',1,0)"
      ],
      "metadata": {
        "id": "8UAwrtyGzwGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Using onehot technique\n",
        "df = pd.get_dummies(df,columns = ['education'])"
      ],
      "metadata": {
        "id": "bNSOIRQuYTSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used one hot encoding technique to the education variable and manually encoded on sex and is_smoking variables.Remaining all the categorical variables are having binary values."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# using VIF technique\n",
        "def calc_vif(X):\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont_df = pd.DataFrame(df[cont_var])\n",
        "cont_df"
      ],
      "metadata": {
        "id": "1UFHQJC20Xuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df[[i for i in cont_df]])"
      ],
      "metadata": {
        "id": "zZ_PSa71cqes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "2R2hoEHpmlR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a new column pulse pressure by taking difference of sys_bp - dia_bp\n",
        "df['pulse_press'] = df['sys_bp'] - df['diaBP']\n",
        "df['pulse_press']"
      ],
      "metadata": {
        "id": "jb_Uj_zGmVaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping the two variables of sys_bp and dia_bp\n",
        "df.drop(columns = ['sys_bp','diaBP'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "jRNK2vEmm5jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # updating the cont,cat lists\n",
        "cont_var.remove('diaBP')\n",
        "cont_var.remove('sys_bp')\n",
        "cont_var.append('pulse_press')"
      ],
      "metadata": {
        "id": "Z_hG5u8wm5aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after updating the features\n",
        "cont_var"
      ],
      "metadata": {
        "id": "uJkdGF00q_vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# updating our continuous df after the feature updates\n",
        "cont_df = pd.DataFrame(df[cont_var])"
      ],
      "metadata": {
        "id": "6JZVGXcTr6Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont_df.columns"
      ],
      "metadata": {
        "id": "eR7YvaF324vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df[[i for i in cont_var]])"
      ],
      "metadata": {
        "id": "l3AQx_s910lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "df.drop('is_smoking', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_var.remove('is_smoking')\n",
        "cat_var"
      ],
      "metadata": {
        "id": "BVWXCnvS6TkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The df after dropping the column\n",
        "df.columns"
      ],
      "metadata": {
        "id": "IMEs6zpa6eDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We utilized the Variance Inflation Factor (VIF) to address multicollinearity within the dataset. During this process, we identified high VIF values for the systolic and diastolic blood pressure columns. To mitigate this issue, we engineered a new feature known as \"pulse pressure.\"\n",
        "\n",
        "Furthermore, we observed that the \"is_smoking\" column merely contained binary values indicating whether an individual smoked or not. This information was redundantly captured in the \"cigs per day\" column, where non-smokers were represented by 0 and smokers by the number of cigarettes smoked per day. As a result, we opted to remove the \"is_smoking\" column to streamline our dataset and eliminate redundancy."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following variables'age', 'sex', 'cigs_per_day', 'bp_meds', 'prevalent_stroke', 'prevalent_hyp', 'diabetes', 'total_chol', 'bmi', 'heart_rate', 'glucose', 'ten_year_chd', 'education_1.0', 'education_2.0', 'education_3.0', 'education_4.0', 'pulse_press'are deemed important. These variables collectively represent demographic, behavioral, medical, and historic medical data."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Checking the skewness of the day\n",
        "df[cont_var].skew(axis = 0)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Skewness for square root transformation\n",
        "np.sqrt(df[cont_var]).skew(axis = 0)"
      ],
      "metadata": {
        "id": "yVU2D5GGIEiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Skewness for log10 transformation\n",
        "np.log10(df[cont_var]+1).skew(axis = 0)"
      ],
      "metadata": {
        "id": "xesbH6fzRis3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying the square transfomations on the continuous variables\n",
        "df['age']             = np.log10(df['age'] + 1)\n",
        "df['cigs_per_day']    = np.log10(df['cigs_per_day'] + 1 )\n",
        "df['total_chol']      = np.sqrt(df['total_chol'])\n",
        "df['bmi']             = np.log10(df['bmi'] + 1)\n",
        "df['heart_rate']      = np.log10(df['heart_rate'] + 1)\n",
        "df['glucose']         = np.log10(df['glucose'] + 1)\n",
        "df['pulse_press']     = np.log10(df['pulse_press'] + 1)"
      ],
      "metadata": {
        "id": "q_bsMhzwSHeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#skewness after tansformation\n",
        "df[cont_var].skew(axis = 0)"
      ],
      "metadata": {
        "id": "B_V5ReUlVB4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, we addressed the issue of data skewness by applying both log and square transformations to our dataset. This process was undertaken to effectively reduce the skewness within the data."
      ],
      "metadata": {
        "id": "v0LLVU39V1KU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaling your data\n",
        "# using standard scaler to the  data\n",
        "sc = StandardScaler()\n",
        "df[cont_var] = sc.fit_transform(df[cont_var])"
      ],
      "metadata": {
        "id": "frl0QH_cXZQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the X and y\n",
        "X = df.drop('ten_year_chd',axis = 1)\n",
        "y = df['ten_year_chd']"
      ],
      "metadata": {
        "id": "B-Ua-y74YJzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the continuous independent variables in the dataset, we have used standard scaler method to scale them into one scale."
      ],
      "metadata": {
        "id": "FFx-GKPBX0Ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test i 80 : 20 ratio\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)\n",
        "print(X_train.shape[0])\n",
        "print(X_test.shape[0])"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}