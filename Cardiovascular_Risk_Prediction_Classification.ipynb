{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "K5QZ13OEpz2H",
        "448CDAPjqfQr",
        "OB4l2ZhMeS1U",
        "dJ2tPlVmpsJ0",
        "-jK_YjpMpsJ2",
        "VaLtwTcOQc1o",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jamal-1018/Cardiovascular_Risk_Prediction_Classification/blob/main/Cardiovascular_Risk_Prediction_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Cardiovascular Risk Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - **Classification**\n",
        "##### **Contribution**    - **Individual**\n",
        "##### **Team Member**     - **Mohammad Jamaluddin**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime as dt\n",
        "\n",
        "#for plotting charts\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# for statistical tests\n",
        "from scipy.stats import chi2_contingency,chi2\n",
        "\n",
        "# for imputing the missing values\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "\n",
        "# For multicolinearity of VIF technique\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# For train test of the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For scaling the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# For handling imbalanced dataset\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "# For metrics of the model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report\n",
        "\n",
        "# For Algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "# For cross validation and hyper parameter tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DteJ9jolrZf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "path = '/content/drive/MyDrive/ALMABETTER CAPSTONE PROJECTS/CLASSIFICATION_cardiovascular_Risk_prediction/data_cardiovascular_risk.csv'\n",
        "\n",
        "DATA = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "DATA.head(5)  #First 5 rows"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data last 5 rows\n",
        "DATA.tail(5)"
      ],
      "metadata": {
        "id": "yH3xQIiQsCBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f'The number of rows in the data is : {DATA.shape[0]}')\n",
        "print(f'The number of columns in the data is : {DATA.shape[1]}')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "DATA.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating copy of dataset\n",
        "df = DATA.copy()"
      ],
      "metadata": {
        "id": "jTHxG0S96BX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull())"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dataset have 3390 rows and 17 columns and the TenYearCHd is dependent variable.\n",
        "- It does not have any duplicate rows.\n",
        "\n",
        "- It contains missing values in education,CigsPerDay,BPMeds.totChol,BMI,glucose columns."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include = 'all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demographic:\n",
        "\n",
        "- id : id of the patitent\n",
        "\n",
        "- age : age of the patient\n",
        "\n",
        "- education : education of the patient\n",
        "\n",
        "- sex : sex of the patient (M/F)\n",
        "\n",
        "Behaviour:\n",
        "\n",
        "- is_smoking : If the patient has the habit of smoking (YES/NO)\n",
        "\n",
        "- cigsPerDay : The average number of ciagrettes that the patient smokes `continuous`\n",
        "\n",
        "Medical History:\n",
        "\n",
        "- BPMeds : Whether or not the patient takes the Blood Pressure medication or not (1/0)\n",
        "\n",
        "- prevalentStroke : Whether or not  the patient has any history of stroke (1/0)\n",
        "\n",
        "- prevalentHyp : Whether or not the patient has any history of hypertension (1/0)\n",
        "\n",
        "- diabetes : WHether or not the patient has diabetes (1/0)\n",
        "\n",
        "Medical (Current) :\n",
        "\n",
        "- totChol : Total Cholestrol level `Continuous`\n",
        "\n",
        "- sysBP : siastol levels in blood pressure `Continuous`\n",
        "\n",
        "- diaBP : Diastol levels in blood pressure `continuous`\n",
        "\n",
        "- BMI : Body Mass Index of the patient `Continuous`\n",
        "\n",
        "- heartrate : Heart Rate of the patient `Continuous`\n",
        "\n",
        "- glucose : Glucose levels of the patient `continuous`\n",
        "\n",
        "Target variable:\n",
        "\n",
        "- TenYearCHD : 10 year Coronary Heart Disease (1: Yes, 0: No) `dependent varaible`"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(f'the number of unique values in {i} is {df[i].nunique()}')"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping id column\n",
        "df.drop('id', axis = 1, inplace = True )"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# renaming the columns\n",
        "df.rename(columns = {'cigsPerDay': 'cigs_per_day','BPMeds':'bp_meds','prevalentStroke': 'prevalent_stroke',\n",
        "           'prevalentHyp' : 'prevalent_hyp','totChol' : 'total_chol', 'sysBP' : 'sys_bp',\n",
        "                     'diaBp' : 'dia_bp','BMI': 'bmi','heartRate': 'heart_rate','TenYearCHD' : 'ten_year_chd'},inplace = True)"
      ],
      "metadata": {
        "id": "ffEfZTbv9Tj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After renaming the columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "QAHfjZmD_GsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categorizing the variables\n",
        "dep_var = ['ten_year_chd']\n",
        "dep_var"
      ],
      "metadata": {
        "id": "s2PKBvTc-yhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# continuous variables\n",
        "cont_var = ['age','cigs_per_day','total_chol','sys_bp','diaBP','bmi','heart_rate','glucose']\n",
        "cont_var"
      ],
      "metadata": {
        "id": "_Cnf9OUp_nkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical variables\n",
        "cat_var = [i for i in df.columns if i not in cont_var]\n",
        "cat_var.remove('ten_year_chd')\n",
        "cat_var"
      ],
      "metadata": {
        "id": "QsJQ1n-z_nnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Column names have been renamed for the ease of use\n",
        "\n",
        "- Categorized the continuous, dependent and cateforical variabes for futher analysis"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 Visualization on dependent variable `ten_year_chd`"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting countplot on dependent variables\n",
        "plt.figure(figsize = (8,8))\n",
        "ax = sns.countplot(data = df, x = df['ten_year_chd'])\n",
        "total = len(df.ten_year_chd)\n",
        "for p in ax.patches:\n",
        "  ax.annotate(f'{100* p.get_height()/total:.1f}%',(p.get_x() + p.get_width()/2,\n",
        "              p.get_height()), ha ='center', va = 'center', fontsize = 12)\n",
        "\n",
        "plt.xlabel('CHD Risk (0: No CHD, 1: CHD)')\n",
        "plt.ylabel('Count')\n",
        "plt.title('CHD(%)')\n",
        "plt.show()\n",
        "\n",
        "print(f'The No risk of CHD is: {df.ten_year_chd.value_counts()[0]}')\n",
        "print(f'The Risk of CHD is: {df.ten_year_chd.value_counts()[1]}')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A countplot is a type of bar chart used in data visualization to display the count or frequency of categorical data. It's a straightforward and effective way to represent the distribution of categorical variables.We used this chart to represent the dependent variables and its distribution."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, we have the following insights:\n",
        "- The values representing 'No risk of CHD' account for the majority at 84.9%, with a total of 2,879 occurrences i.e negative.\n",
        "- Conversely, the values representing 'Risk of CHD' are in the minority at 15.1%, totaling 511 occurrences i.e positive."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classification of patients into positive and negative categories based on their health risk is crucial for building predictive models. This classification provides valuable insights for healthcare industries, guiding the development of effective strategies and interventions. However, it's essential to note that failing to address a high risk of CHD in a patient could have a negative impact on both the patient's health and the healthcare businesses as a whole."
      ],
      "metadata": {
        "id": "5x6oNbhS20hK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2  Visualization of distribution and box plots on continuous variables"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting of distribution ad boxplots\n",
        "for col in df.describe().columns:\n",
        "  fig,axes = plt.subplots(nrows = 1, ncols =2 , figsize = ( 16,5))\n",
        "  sns.histplot(df[col], ax = axes[0],kde = True)\n",
        "  sns.boxplot(df[col], ax = axes[1],showmeans = True, color = 'orange')\n",
        "  fig.suptitle(f'The distribution of {col}')\n",
        "  plt.show()\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "kjAvY2QNCO3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histplot is a type of chart that displays the distribution of a dataset. It is a graphical representation of the data that shows how often each value or group of values occurs. Histplots are useful for understanding the distribution of a dataset and identifying patterns or trends in the data. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data.\n",
        "\n",
        "Thus, I used the histogram plot to analysis the variable distributions over the whole dataset whether it's symmetric or not.\n",
        "\n",
        "A boxplot is used to summarize the key statistical characteristics of a dataset, including the median, quartiles, and range, in a single plot. Boxplots are useful for identifying the presence of outliers in a dataset, comparing the distribution of multiple datasets, and understanding the dispersion of the data. They are often used in statistical analysis and data visualization.\n",
        "\n",
        "Thus, for each numerical varibale in the given dataset, I used box plot to analyse the outliers and interquartile range including mean, median, maximum and minimum value."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, we have observed that the majority of the variables in the dataset exhibit a normal distribution. However, some variables appear to be skewed, indicating a departure from normality. Additionally, there are outliers present in certain variables as well"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These insights are important for understanding the characteristics of the data and can inform our data preprocessing and modeling decisions."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 Visulaization of categorical variables"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['ten_year_chd'].value_counts()"
      ],
      "metadata": {
        "id": "zWUbfGO8jJ4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting of counter plot on categorical columns\n",
        "for i in cat_var:\n",
        "  plt.figure(figsize = (10,5))\n",
        "  p = sns.countplot(x = df[i], data =df)\n",
        "  total_len = len(df[i])\n",
        "  for i in p.patches:\n",
        "    p.annotate(f'{(100 * i.get_height()/total_len):.2f}%',((i.get_x()+i.get_width()),(i.get_height())),\n",
        "               ha = 'center', va = 'center', fontsize = 10)\n",
        "  plt.show()\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A countplot is a type of bar chart used in data visualization to display the count or frequency of categorical data. It's a straightforward and effective way to represent the distribution of categorical variables.Here we used this char on all categorical columns to represent the distribution of the data."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, the following insights are observed:\n",
        "\n",
        "- The 'education' variable is most prominently represented by category 1, accounting for 41.03% of the data, while the other categories each have less than 29% representation.\n",
        "\n",
        "- The 'sex' variable exhibits a well-balanced distribution, with 43.2% for males and 56.7% for females.\n",
        "\n",
        "- The 'smoker' variable also demonstrates a balanced distribution, with 49.76% being smokers and 50.24% non-smokers.\n",
        "\n",
        "- On the other hand, variables such as 'bp_meds,' 'prevalent_stroke,' 'prevalent_hypertension,' and 'diabetes' show an imbalanced distribution of data."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights have the potential to create a positive business impact.However, the extent of the impact depends on how these insights are leveraged and applied to meet specific business objectives. Understanding the distribution of variables is a crucial step in data-driven decision-making and can lead to more informed and effective strategies."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Visualization of categorical variables and distribution"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting a categorical plot on all categorical variables\n",
        "for i in cont_var:\n",
        "  plt.figure(figsize = (10,8))\n",
        "  sns.catplot(x = dep_var[0], y = i, data = df, kind = 'violin')\n",
        "  plt.title(dep_var[0]+' vs '+ i)\n",
        "  plt.show()\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The violin chart is a type of data visualization that combines aspects of a box plot and a kernel density plot. It is used to display the distribution and density of data across different categories or groups. Some features of the violin chart include:\n",
        "\n",
        "- Shape and width: The shape of the violin represents the data distribution, typically displaying a mirrored, symmetrical shape. The width of the violin at different points indicates the density of data.\n",
        "\n",
        "- Quartiles and median: The central \"box\" in the violin chart represents the interquartile range (IQR) and contains the median value. This provides insights into the spread and central tendency of the data.\n",
        "\n",
        "Grouping and comparison: Violin charts can be grouped or arranged side by side to compare distributions across different categories or groups. This allows for visual comparisons of data distribution shapes, spreads, and densities.\n",
        "\n",
        "Here, we used this chart on all continuous variables with dependent variable to observe the distribution and central tendency of the data."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the insights:\n",
        "\n",
        "-  Instances of a positive risk of CHD are higher for individuals with age values greater than 50 when compared to those with a negative risk of CHD.\n",
        "\n",
        "- The density of 'cigsperday' values is higher in occurrences with a negative risk of CHD as opposed to those with a positive risk of CHD.\n",
        "\n",
        "- The density of 'glucose' values is higher in occurrences with a negative risk of CHD as opposed to those with a positive risk of CHD."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the violin chart can potentially create a positive business impact by providing valuable information for decision-making. Understanding the distribution and density of data across different categories can help businesses identify patterns, trends, and potential areas of focus. For example, the insight that positive cases of ten-year CHD are higher in older people suggests the need for targeted preventive measures or specialized treatments for this demographic. Similarly, the insight regarding the relationship between smoking and negative cases of CHD can inform smoking cessation programs or campaigns to reduce the risk of CHD.\n",
        "\n",
        "While the insights gained from the chart can be valuable, it's important to note that the impact on business growth would depend on various factors. The actual business impact would require further analysis and strategic implementation of these insights. Additionally, without specific business context and objectives, it is challenging to determine if there are any insights that would directly lead to negative growth. However, using the insights to better understand the distribution of health conditions and risk factors can potentially help businesses in the healthcare industry develop more effective strategies and interventions to improve patient outcomes and drive positive growth."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 Visualization of categorical variables with stacked bar charts"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting of stacked bar charts on categorical variables to observe percenatage of occurence of the dependent variables\n",
        "for col in cat_var:\n",
        "  grouped_data = df.groupby(col)['ten_year_chd'].value_counts(normalize  =True).unstack('ten_year_chd') * 100\n",
        "  plt.figure(figsize = (10,8))\n",
        "  grouped_data.plot.barh(stacked = True)\n",
        "\n",
        "# plotting the graph to show percentages\n",
        "  for ix, row in grouped_data.reset_index(drop = True).iterrows():\n",
        "    cummulative = 0\n",
        "    for element in row:\n",
        "      if element > 0.1:\n",
        "        plt.text(cummulative + element /2 ,\n",
        "                 ix,\n",
        "                 f\"{int(element)} %\",\n",
        "                 ha = 'center',\n",
        "                 va = 'center')\n",
        "      cummulative += element\n",
        "  plt.show()\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A horizontal stacked bar chart is a type of data visualization designed to illustrate the composition or proportion of various categories within a whole.The chart presents multiple categories or groups stacked horizontally, enabling straightforward visual comparisons of their relative proportions within the total. Each bar represents the whole, with its segments depicting the different categories or components."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, these are the below insights:\n",
        "- The percentage of CHD is slightly higher in the 'education' variable's class 1.0 when compared to other classes of the variable.\n",
        "\n",
        "- For the 'sex' and 'is_smoker' variables, it's noticeable that the CHD risk is slightly greater in the 'Male' class and 'Smoker: Yes' class.\n",
        "\n",
        "- Among the other variables ('bp_meds,' 'prevalent_stroke,' 'prevalent_hypertension,' and 'diabetes'), the percentage of CHD risk is significantly higher for the 'Yes' classes when compared to the 'No' classes, respectively."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the distribution and composition of different categories in relation to the occurrence of CHD. These insights can guide businesses in developing targeted strategies and interventions to address specific demographic groups or risk factors. For example, businesses can design preventive measures, educational campaigns, or tailored treatments to reduce the occurrence of CHD in high-risk categories. By leveraging these insights, businesses in the healthcare industry can improve patient outcomes, enhance customer satisfaction, and drive positive growth.\n",
        "\n",
        "There are no specific insights from the chart that directly lead to negative growth. However, it's important to consider the overall prevalence of CHD and the effectiveness of interventions. If the prevalence of CHD remains high across all categories and the implemented strategies fail to yield desired outcomes, it could potentially result in negative growth due to increased healthcare costs, decreased patient satisfaction, or reputational issues. Therefore, the business impact ultimately depends on the successful implementation of strategies based on the insights gained from the chart."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 Correlation Heatmap"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting of heatmap\n",
        "corr = df.corr()\n",
        "mask = np.zeros_like(corr)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "plt.figure(figsize = (18,9))\n",
        "sns.heatmap(corr,mask = mask, annot =True, vmin = -1,vmax = 1,cmap = \"coolwarm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8ollOuc9bcwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting bar chart to find the correlation of every independent variable with dependent varable\n",
        "df.drop('ten_year_chd',axis = 1).corrwith(df['ten_year_chd']).plot(kind = 'bar',grid = True,figsize = (8,3))\n",
        "plt.ylim(-1.0,1.0)"
      ],
      "metadata": {
        "id": "bqNCi7xTzGxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation coefficient is a measure of the strength and direction of a linear relationship between two variables. A correlation matrix is used to summarize the relationships among a set of variables and is an important tool for data exploration and for selecting which variables to include in a model.\n",
        "\n",
        "The range of correlation is [-1,1].\n",
        "\n",
        "- If the value is towards 1, then there is a positive correlation which means if one variable value increases then the other value also increases.\n",
        "- If the value is towards -1 then there is negative correlation which means if one value increase then the other value decreases.\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, we have used correlation heatmap."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, below insights are:\n",
        "- The 'sys_bp' and 'diaBP' are strongly positively correlated with a coefficient of 0.78.\n",
        "- 'Diabetes' and 'glucose' exhibit a slightly positive correlation with a coefficient of 0.68.\n",
        "- 'sysBP' and 'diaBP' display slightly positive correlations with 'prevalent_hypertension' with coefficients of 0.7 and 0.61, respectively."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 7 -  Plotting of pairplot"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting of pairplot\n",
        "sns.pairplot(df, hue = 'ten_year_chd')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pairplot, also known as a scatterplot matrix, is a visualization that allows you to visualize the relationships between all pairs of variables in a dataset. It is a useful tool for data exploration because it allows you to quickly see how all of the variables in a dataset are related to one another.\n",
        "\n",
        "Thus, we used pair plot to analyse the patterns of data and relationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, below insights are:\n",
        "- The cigs_per_day and education have skewed distributions,so we will perform futher analysis and transformations."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check categorical variables are related with dependent varaible\n",
        "- Null hypothesis (H0) : categorical and dependent variables are independent\n",
        "\n",
        "- Alternate hypothesis (H1): categorical and dependent variables are not independent"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value(Chi2)\n",
        "def chi_metric(dataframe):\n",
        "\n",
        "  p_values = []\n",
        "  chi_squared_values = []\n",
        "  for column in dataframe.columns:\n",
        "    cross_tab = pd.crosstab(df[column], df['ten_year_chd'])\n",
        "    chi2, p, dof, expected = chi2_contingency(cross_tab)  #Using chi2 contingency table\n",
        "    p_values.append(p)\n",
        "    chi_squared_values.append(chi2)\n",
        "\n",
        "    print(f'Analyzing variable: {column}')\n",
        "    print(f'Chi-squared: {chi2}')\n",
        "    print(f'p-value: {p}')\n",
        "    print('-' * 20)\n",
        "  print(p_values)\n",
        "\n",
        "# Plotting the p_value of the category varaibles\n",
        "  pd.Series(p_values, index = dataframe.columns).plot.barh()\n",
        "  plt.xscale('log')\n",
        "  plt.title('P_value for categorical variables')\n",
        "  plt.xlabel('P_value')\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat = df[cat_var]\n",
        "chi_metric(df_cat)"
      ],
      "metadata": {
        "id": "wazGx99yckle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p_value for all the variables are lesser than the p_value, so we reject null hypothesis."
      ],
      "metadata": {
        "id": "Ji4G-nguf-lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the hypothesis that the categorical and dependent variables are independent or not,we performed a chi-squared test of independence. This statistical test allowed to determine the relationship. By calculating the chi-squared statistic and p-value, we can make a statistical inference about the relationship between the variables in the dataset."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We performed the chi-squared test of independence to test the hypothesis that the categorical variables and dependent variable are not independent to each other because it is an appropriate statistical test for determining if there is a significant association between two categorical variables. In this case, both variabels are categorical variables, so the chi-squared test is a suitable choice.\n",
        "\n",
        "The chi-squared test works by comparing the observed frequency distribution of the data in a contingency table to the expected frequency distribution under the assumption that the null hypothesis is true. If there is a significant difference between the observed and expected frequencies, it suggests that there is a relationship between the two variables.\n",
        "\n",
        "Overall, we used the chi-squared test of independence because it is a widely used and well-established statistical test for analyzing the relationship between two categorical variables. It allowed us to make a statistical inference about the relationship between categorical variables and dependent variables."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a copy of my dataset 2\n",
        "df2 = df.copy()\n"
      ],
      "metadata": {
        "id": "rhrEgM1MG__3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# variables which have missing/Nan values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Handling missing values in categorical varibles"
      ],
      "metadata": {
        "id": "E6WlncjRtIJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replacing the null values with mode values of the categorical variables\n",
        "df['bp_meds'] = df['bp_meds'].fillna(df['bp_meds'].mode()[0])\n",
        "df['education'] = df['education'].fillna(df['education'].mode()[0])"
      ],
      "metadata": {
        "id": "qFKG8oZ7tR5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Handling the missing values in continous variables of `cig_per_day`"
      ],
      "metadata": {
        "id": "NRU156O2ukIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mean and median of the cigs per day\n",
        "df['cigs_per_day'].mean().round(0), df['cigs_per_day'].median()"
      ],
      "metadata": {
        "id": "U_dN_rGAun7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['cigs_per_day'].isnull()][['is_smoking','cigs_per_day']]"
      ],
      "metadata": {
        "id": "W_emrw-wvK-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can observe that for every NaN value, the is_smoking value is YES. which assures that cigs per day variable is not 0."
      ],
      "metadata": {
        "id": "rNiEvdXAvwHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the mean and median of cigs per day excluding non smokers\n",
        "df[df['is_smoking'] == 'YES']['cigs_per_day'].mean(),df[df['is_smoking'] == 'YES']['cigs_per_day'].median()"
      ],
      "metadata": {
        "id": "1VzkP2T1wGis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputting with the median values in the NaN values\n",
        "df['cigs_per_day'] = df['cigs_per_day'].fillna(df[df['is_smoking'] == 'YES']['cigs_per_day'].median())"
      ],
      "metadata": {
        "id": "bGX9srqOwoVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for missing entries like where patient is_smoking is yes & cigs per day is 0\n",
        "df[(df['is_smoking'] == 'YES') & (df['cigs_per_day'] == 0)]"
      ],
      "metadata": {
        "id": "WX-LqVHVwoas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Handling the total cholestrol,bmi,heartrate variable"
      ],
      "metadata": {
        "id": "4wD5p2fAxpLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mean,median of total cholestrol variable\n",
        "df['total_chol'].mean(),df['total_chol'].median()"
      ],
      "metadata": {
        "id": "q5fU27PhxxTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputing with the median values in the NaN values\n",
        "df['total_chol'] = df['total_chol'].fillna(df['total_chol'].median())"
      ],
      "metadata": {
        "id": "1Fi-Q103yY6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean,median of bmi variable\n",
        "df['bmi'].mean(), df['bmi'].median()"
      ],
      "metadata": {
        "id": "jJ962KXlwod0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputting with the median variable\n",
        "df['bmi'] = df['bmi'].fillna(df['bmi'].median())"
      ],
      "metadata": {
        "id": "F573vja6y_pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean, median of heart rate variables\n",
        "df['heart_rate'].mean(), df['heart_rate'].median()"
      ],
      "metadata": {
        "id": "Wtbajdaoy_s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing with the median values of the variable\n",
        "df['heart_rate'] = df['heart_rate'].fillna(df['heart_rate'].median())"
      ],
      "metadata": {
        "id": "oWHB2sWu0fTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Handling glucose variable"
      ],
      "metadata": {
        "id": "crLLBuee0zMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['glucose'].mean().round(0), df['glucose'].median()"
      ],
      "metadata": {
        "id": "H-xHws6G0y2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The data spread being skewed positively and values falling outside the typical range, a more careful approach is needed, especially for the glucose column with 304 missing data points. Simply using the mean or median for imputation may introduce significant inaccuracies.\n",
        "\n",
        "- One effective solution is to employ the KNN imputer method. This method considers the relationships between data points and is more suitable for imputing missing values, ensuring that the imputed values align better with the overall data distribution."
      ],
      "metadata": {
        "id": "ZU5eyh9u3Vq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying KNNImputer to the glucose variable\n",
        "imputer = KNNImputer(n_neighbors = 5)\n",
        "imputed_column = imputer.fit_transform(df[['glucose']])\n",
        "df['glucose'] = imputed_column"
      ],
      "metadata": {
        "id": "hYzFNn7X2zSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After imputing, mean and median of the column\n",
        "df['glucose'].mean(), df['glucose'].median()"
      ],
      "metadata": {
        "id": "z20z8Yk35AFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for the missing/null values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "WB9rW4y_5yBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our data preprocessing, we've employed multiple imputation techniques to address missing values in the dataset. Here's how we've applied these techniques:\n",
        "\n",
        "1. **Median Imputer for Skewed Continuous Variables:** For continuous variables that exhibited skewness, we opted for the median imputation technique. The median is a robust measure of central tendency that is not influenced by outliers, making it a reliable choice for filling missing values in skewed data.\n",
        "\n",
        "2. **Mode Imputer for Categorical Variables:** When dealing with categorical variables, we utilized the mode imputation technique. The mode represents the most common and frequent value within a category, making it a suitable estimate for handling missing values in categorical data.\n",
        "\n",
        "3. **KNN Imputer for Continuous Variables:** We used KNN Imputer technique that imputes missing values using the k-nearest neighbors algorithm. It replaces missing values by taking into account the values of the k-nearest neighbors of the missing data point. This method can be particularly useful when there is a complex relationship between features, and simple imputation methods like mean or median are not appropriate.\n",
        "\n",
        "By selecting appropriate imputation techniques based on the data type and characteristics of the variables, we aimed to ensure the robustness and accuracy of the imputed values while preparing the dataset for analysis.\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers with IQR(Inter Quartile Range) technique\n",
        "for col in cont_var:\n",
        "  q1, median, q3 = df[col].quantile([0.25,0.50,0.75])\n",
        "  iqr = q3 - q1\n",
        "  upper_limit = q3 + 1.5 * iqr\n",
        "  lower_limit = q1 - 1.5 * iqr\n",
        "\n",
        "  # Replacing with the outliers\n",
        "  df[col] = np.where(df[col] > upper_limit, upper_limit, df[col])\n",
        "  df[col] = np.where(df[col] < lower_limit, lower_limit, df[col])\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Applying the IQR function on continuous variables of the data\n",
        "df[cont_var]"
      ],
      "metadata": {
        "id": "wBMuudeh-jvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have applied the Interquartile Range (IQR) method to identify and eliminate outliers present in the continuous columns of the dataset. This method was chosen due to its robustness in outlier detection, as it remains unaffected by extreme values. The IQR is computed as the difference between the 75th and 25th percentiles of the data, and any data point falling below the 25th percentile minus 1.5 times the IQR or exceeding the 75th percentile plus 1.5 times the IQR is categorized as an outlier. By utilizing the IQR method, We ensured a consistent and objective approach to identify and remove outliers from the dataset."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Encoding categorical variables\n",
        "df['sex'] = np.where(df['sex'] == 'M',1,0)\n",
        "df['is_smoking'] = np.where(df['is_smoking'] == 'YES',1,0)"
      ],
      "metadata": {
        "id": "8UAwrtyGzwGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Using onehot technique\n",
        "df = pd.get_dummies(df,columns = ['education'])"
      ],
      "metadata": {
        "id": "bNSOIRQuYTSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used one hot encoding technique to the education variable and manually encoded on sex and is_smoking variables.Remaining all the categorical variables are having binary values."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# using VIF technique\n",
        "def calc_vif(X):\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont_df = pd.DataFrame(df[cont_var])\n",
        "cont_df"
      ],
      "metadata": {
        "id": "1UFHQJC20Xuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df[[i for i in cont_df]])"
      ],
      "metadata": {
        "id": "zZ_PSa71cqes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "2R2hoEHpmlR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a new column pulse pressure by taking difference of sys_bp - dia_bp\n",
        "df['pulse_press'] = df['sys_bp'] - df['diaBP']\n",
        "df['pulse_press']"
      ],
      "metadata": {
        "id": "jb_Uj_zGmVaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping the two variables of sys_bp and dia_bp\n",
        "df.drop(columns = ['sys_bp','diaBP'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "jRNK2vEmm5jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # updating the cont,cat lists\n",
        "cont_var.remove('diaBP')\n",
        "cont_var.remove('sys_bp')\n",
        "cont_var.append('pulse_press')"
      ],
      "metadata": {
        "id": "Z_hG5u8wm5aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after updating the features\n",
        "cont_var"
      ],
      "metadata": {
        "id": "uJkdGF00q_vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# updating our continuous df after the feature updates\n",
        "cont_df = pd.DataFrame(df[cont_var])"
      ],
      "metadata": {
        "id": "6JZVGXcTr6Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont_df.columns"
      ],
      "metadata": {
        "id": "eR7YvaF324vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df[[i for i in cont_var]])"
      ],
      "metadata": {
        "id": "l3AQx_s910lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "df.drop('is_smoking', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_var.remove('is_smoking')\n",
        "cat_var"
      ],
      "metadata": {
        "id": "BVWXCnvS6TkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The df after dropping the column\n",
        "df.columns"
      ],
      "metadata": {
        "id": "IMEs6zpa6eDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We utilized the Variance Inflation Factor (VIF) to address multicollinearity within the dataset. During this process, we identified high VIF values for the systolic and diastolic blood pressure columns. To mitigate this issue, we engineered a new feature known as \"pulse pressure.\"\n",
        "\n",
        "Furthermore, we observed that the \"is_smoking\" column merely contained binary values indicating whether an individual smoked or not. This information was redundantly captured in the \"cigs per day\" column, where non-smokers were represented by 0 and smokers by the number of cigarettes smoked per day. As a result, we opted to remove the \"is_smoking\" column to streamline our dataset and eliminate redundancy."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following variables—'age', 'sex', 'cigs_per_day', 'bp_meds', 'prevalent_stroke', 'prevalent_hyp', 'diabetes', 'total_chol', 'bmi', 'heart_rate', 'glucose', 'ten_year_chd', 'education_1.0', 'education_2.0', 'education_3.0', 'education_4.0', 'pulse_press'—are deemed important. These variables collectively represent demographic, behavioral, medical, and historic medical data."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Checking the skewness of the day\n",
        "df[cont_var].skew(axis = 0)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Skewness for square root transformation\n",
        "np.sqrt(df[cont_var]).skew(axis = 0)"
      ],
      "metadata": {
        "id": "yVU2D5GGIEiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Skewness for log10 transformation\n",
        "np.log10(df[cont_var]+1).skew(axis = 0)"
      ],
      "metadata": {
        "id": "xesbH6fzRis3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying the square transfomations on the continuous variables\n",
        "df['age']             = np.log10(df['age'] + 1)\n",
        "df['cigs_per_day']    = np.log10(df['cigs_per_day'] + 1 )\n",
        "df['total_chol']      = np.sqrt(df['total_chol'])\n",
        "df['bmi']             = np.log10(df['bmi'] + 1)\n",
        "df['heart_rate']      = np.log10(df['heart_rate'] + 1)\n",
        "df['glucose']         = np.log10(df['glucose'] + 1)\n",
        "df['pulse_press']     = np.log10(df['pulse_press'] + 1)"
      ],
      "metadata": {
        "id": "q_bsMhzwSHeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#skewness after tansformation\n",
        "df[cont_var].skew(axis = 0)"
      ],
      "metadata": {
        "id": "B_V5ReUlVB4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, we addressed the issue of data skewness by applying both log and square transformations to our dataset. This process was undertaken to effectively reduce the skewness within the data."
      ],
      "metadata": {
        "id": "v0LLVU39V1KU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaling your data\n",
        "# using standard scaler to the  data\n",
        "sc = StandardScaler()\n",
        "df[cont_var] = sc.fit_transform(df[cont_var])"
      ],
      "metadata": {
        "id": "frl0QH_cXZQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the X and y\n",
        "X = df.drop('ten_year_chd',axis = 1)\n",
        "y = df['ten_year_chd']"
      ],
      "metadata": {
        "id": "B-Ua-y74YJzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the continuous independent variables in the dataset, we have used standard scaler method to scale them into one scale."
      ],
      "metadata": {
        "id": "FFx-GKPBX0Ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test i 80 : 20 ratio\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)\n",
        "print(f'The number of rows for training: {X_train.shape[0]}')\n",
        "print(f'The number of rows for testing:  {X_test.shape[0]}')"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split our data into training and testing sets using the `train_test_split` method. In this process, we allocated 80% of the data to the training set and 20% to the testing set. This split ratio strikes a balance between providing ample data for effective model training and reserving a portion for evaluating the model's performance on unseen data. By dedicating 80% of the data to training, our model gains access to a substantial amount of information to learn from, while the remaining 20% is reserved for assessing the model's ability to generalize to new, unseen data."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, our dataset exhibits an imbalance in the distribution of classes. Specifically, the number of instances representing a positive risk of Coronary Heart Disease (CHD) is notably lower than the occurrences indicating no risk of CHD."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# plotting a bar chart for dependent variable\n",
        "y_train.value_counts().plot(kind = 'bar', title = 'Imbalanced data in dependent variable')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the smote to balance the dataset\n",
        "X_smote,y_smote = SMOTETomek(random_state = 42).fit_resample(X_train,y_train)\n",
        "print(f'samples in origina dataset {y_train.shape[0]}')\n",
        "print(f'samples in resampled dataset {y_smote.shape[0]}')"
      ],
      "metadata": {
        "id": "uPMGiCGv_q4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#after balancing the data\n",
        "y_smote.value_counts().plot(kind = 'bar', title = 'Data after SMOTE technique')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nyl8gqlmCF1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As our dependent variable is unbalanced classification, so we used smotetomek techniue to balance the data.SMOTETomek is a combination of two resampling techniques: SMOTE (Synthetic Minority Over-sampling Technique) and Tomek links. It is used to address the class imbalance problem in machine learning, especially in binary classification tasks where one class is significantly smaller than the other.\n",
        "- SMOTE (Synthetic Minority Over-sampling Technique): SMOTE is an oversampling technique that generates synthetic samples for the minority class. It does this by interpolating between existing minority class instances. SMOTE creates synthetic samples by choosing a random minority class instance and then selecting its k-nearest neighbors. It then generates new instances along the line segments connecting the chosen instance and its neighbors.\n",
        "\n",
        "- Tomek Links: Tomek links are pairs of instances, one from the majority class and one from the minority class, that are very close to each other but belong to different classes. These instances are considered noisy or ambiguous. Removing the instances involved in Tomek links can improve the quality of the dataset."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function for evaluation matrix\n",
        "def evaluation_matrix(model,X_train,X_test,y_train,y_test,features):\n",
        "  \"\"\"This function takes the model, `X_train`, `X_test`, `y_train`, and `y_test` as inputs and performs the following steps:\n",
        "    - Fits the model using the training data.\n",
        "    - Makes predictions on the trained model.\n",
        "    - Calculates and prints the ROC AUC score for both the training and test sets, along with plotting the ROC curves.\n",
        "    - Prints the confusion matrix for both the training and test sets.\n",
        "    - Prints the classification report for both the training and test sets.\n",
        "    - If the model has feature importances, it plots them.\n",
        "    - Provides a list of performance metrics: `recall_train`, `recall_test`, `roc_auc_train`, `roc_auc_test`, `f1_train`, and `f1_test`.\n",
        "\n",
        "      The function performs a comprehensive evaluation of the model's performance\n",
        "      on both the training and test datasets and provides key metrics to assess its quality and generalization.\"\"\"\n",
        "\n",
        "  # Fitting the data to the model\n",
        "  model.fit(X_train,y_train)\n",
        "\n",
        "  # predicting on trained dataset\n",
        "  y_pred_train     = model.predict(X_train)\n",
        "  y_pred_test      = model.predict(X_test)\n",
        "  pred_prob_train  = model.predict_proba(X_train)[:,1]\n",
        "  pred_prob_test   = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "  # printing the scores\n",
        "  roc_auc_train  = roc_auc_score(y_pred_train,y_train)\n",
        "  roc_auc_test     = roc_auc_score(y_pred_test, y_test)\n",
        "  print(f'\\n The ROC AUC score for train : {roc_auc_train}')\n",
        "  print(f'The ROC AUC score for test : {roc_auc_test}')\n",
        "\n",
        "  # plotting ROC curve\n",
        "  fpr_train,tpr_train,train_thresholds  = roc_curve(y_train,pred_prob_train)\n",
        "  fpr_test, tpr_test, test_thresholds   = roc_curve(y_test, pred_prob_test)\n",
        "  plt.plot([0,1],[0,1],'k--')\n",
        "  plt.plot(fpr_train, tpr_train,label = \"Train ROC AUC: {:.2f}\".format(roc_auc_train))\n",
        "  plt.plot(fpr_test, tpr_test, label = \"Test ROC AUC: {:.2f}\".format(roc_auc_test))\n",
        "  plt.legend()\n",
        "  plt.title('ROC curve')\n",
        "  plt.xlabel('False positive rate')\n",
        "  plt.ylabel('True positive rate')\n",
        "  plt.show()\n",
        "\n",
        "  # calculating confusion matrix\n",
        "  cm_train = confusion_matrix(y_train,y_pred_train)\n",
        "  cm_test  = confusion_matrix(y_test,y_pred_test)\n",
        "\n",
        "  # plotting confusion matrx\n",
        "  print(\"\\nConfusion Matrix:\")\n",
        "  fig, ax = plt.subplots(1,2, figsize = (12,4))\n",
        "  sns.heatmap(cm_train, annot = True, xticklabels = ['Negative','Positive'], yticklabels = ['Negative','Positive'], cmap = 'coolwarm',fmt = '.4g', ax = ax[0])\n",
        "  ax[0].set_title('Train confusion matrix')\n",
        "  ax[0].set_xlabel('Predicted labels')\n",
        "  ax[0].set_ylabel('True labels')\n",
        "\n",
        "  sns.heatmap(cm_test, annot =True, xticklabels = ['Negative','Positive'], yticklabels = ['Negative','Positive'], cmap = 'coolwarm', fmt = '.4g', ax = ax[1])\n",
        "  ax[1].set_title('Test confusion matrix')\n",
        "  ax[1].set_xlabel('Predicted labels')\n",
        "  ax[1].set_ylabel('True labels')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Calculating classification report\n",
        "  cr_train = classification_report(y_train,y_pred_train,output_dict = True)\n",
        "  cr_test  = classification_report(y_test, y_pred_test, output_dict = True)\n",
        "  print('\\nTrain Classification Report')\n",
        "  crt = pd.DataFrame(cr_train).T\n",
        "  print(crt.to_markdown())\n",
        "  print('\\nTest Classification Report ')\n",
        "  crt2 = pd.DataFrame(cr_test).T\n",
        "  print(crt2.to_markdown())\n",
        "\n",
        "  # Feature importance\n",
        "  try:\n",
        "    try:\n",
        "      feature_importance = model.feature_importances_\n",
        "    except:\n",
        "      feature_importance = model.coef_\n",
        "    feature_importance = np.absolute(feature_importance)\n",
        "    if len(feature_importance) == len(features):\n",
        "      pass\n",
        "    else:\n",
        "      feature_importance = feature_importance[0]\n",
        "\n",
        "\n",
        "    feat = pd.Series(feature_importance, index=features)\n",
        "    feat = feat.sort_values(ascending = True)\n",
        "    plt.figure(figsize = (10,8))\n",
        "    feat.plot(kind ='barh')\n",
        "    plt.title('Feature importance of ' + str(model),fontsize = (16))\n",
        "    plt.xlabel('Relative Importance')\n",
        "  except AttributeError:\n",
        "    print('\\n This model does not have feature importance attribute.')\n",
        "\n",
        "  precision_train = cr_train['weighted avg']['precision']\n",
        "  precision_test  = cr_test['weighted avg']['precision']\n",
        "\n",
        "  recall_train = cr_train['weighted avg']['recall']\n",
        "  recall_test  = cr_test['weighted avg']['recall']\n",
        "\n",
        "  f1_score_train = cr_train['weighted avg']['f1-score']\n",
        "  f1_score_test  = cr_test['weighted avg']['f1-score']\n",
        "\n",
        "  accuracy_train = accuracy_score(y_train,y_pred_train)\n",
        "  accuracy_test  = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "  model_scores = [precision_train, precision_test,recall_train,recall_test,f1_score_train,f1_score_test,accuracy_train,accuracy_test,roc_auc_train,roc_auc_test]\n",
        "  return model_scores"
      ],
      "metadata": {
        "id": "DmZbqGm9FMCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe of scores\n",
        "scores = pd.DataFrame(index = ['Precision_train','Precision_test','Recall_train','Recall_test','F1_score_train','F1_score_test','Accuracy_train','Accuracy_test','ROC_AUC_train','ROC_AUC_test'])"
      ],
      "metadata": {
        "id": "HMuY8zsJ6tPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 Logistic Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "lr = LogisticRegression(max_iter = 1000, fit_intercept = True)\n",
        "\n",
        "# Model will be fitted and trained in the evaluation matrix function"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "log_reg = evaluation_matrix(lr,X_smote,X_test,y_smote,y_test,X_smote.columns.tolist())\n",
        "scores['Logistic Regression'] = log_reg"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "iKCexbOUTq5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing hyperparameter tuninig\n",
        "param_grid = {'C':[100,10,1,0.1,0.001,0.0001],\n",
        "              'penalty' : ['l1','l2'],\n",
        "              'solver' : ['newton-cg','lbfgs','liblinear','sag','saga']}\n",
        "\n",
        "# initializing the logistic regression\n",
        "log_reg = LogisticRegression(max_iter = 1000, fit_intercept = True, random_state  = 42 )\n",
        "\n",
        "# repeated stratified k fold\n",
        "rpsk = RepeatedStratifiedKFold(n_splits = 3, n_repeats = 4, random_state = 42)\n",
        "\n",
        "#Applying GridSearchcv\n",
        "grid = GridSearchCV(log_reg, param_grid, cv = rpsk)\n",
        "\n",
        "grid.fit(X_smote,y_smote)\n",
        "print(f'The best parameters are : {grid.best_params_}')"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting the best parameters to the model\n",
        "log_reg_best = LogisticRegression(C = grid.best_params_['C'],\n",
        "                    penalty = grid.best_params_['penalty'],\n",
        "                    solver = grid.best_params_['solver'], max_iter = 1000)"
      ],
      "metadata": {
        "id": "A4B8b2rHXmLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing with evaluation matrix\n",
        "log_reg_best_mat = evaluation_matrix(log_reg_best, X_smote,X_test, y_smote,y_test,X_smote.columns.tolist())\n",
        "scores['Logistic Regression tuned'] = log_reg_best_mat"
      ],
      "metadata": {
        "id": "Y8wSNy7kZRot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is a hyperparameter optimization technique that conducts a thorough and systematic search through a predefined parameter grid to identify the optimal hyperparameters for a machine learning model. It is a widely adopted method for hyperparameter tuning due to its simplicity and effectiveness.\n",
        "\n",
        "The selection of a hyperparameter optimization method should take into account several considerations, including the complexity of the parameter space, the available computational resources, and any time constraints. GridSearchCV is particularly suitable when the hyperparameter space is manageable in size and computational resources are not a limiting factor."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "Ftnhr3d4aHSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite hyperparameter tuning efforts, the Logistic Regression model's performance on the test set remained unchanged. The precision, recall, accuracy, ROC-AUC, and F1 scores for both the untuned and tuned models are identical."
      ],
      "metadata": {
        "id": "Fc6TtwtHa31U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 Decision Trees"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the decision trees model\n",
        "dt = DecisionTreeClassifier(random_state = 42)"
      ],
      "metadata": {
        "id": "66nDNEcZbWNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "dt_mat = evaluation_matrix(dt,X_smote,X_test,y_smote,y_test,X_smote.columns.tolist())\n",
        "scores['Decision Trees'] = dt_mat"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "DyOh8iaJcqbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the model for hyperparameter\n",
        "param_grid = {'max_depth' : [3,4,5,7,9],\n",
        "              'min_samples_split' : np.arange(2,8),\n",
        "              'min_samples_leaf' : np.arange(10,20)}\n",
        "\n",
        "# Initializing the model\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# repeated stratifiedKFold\n",
        "rspf = RepeatedStratifiedKFold(n_splits = 3, n_repeats = 4, random_state = 42)\n",
        "\n",
        "# Applying Grisearch\n",
        "grid = GridSearchCV(dt, param_grid, cv = rspf )\n",
        "\n",
        "grid.fit(X_smote,y_smote)\n",
        "\n",
        "# Best parameters\n",
        "print(f'The best parameters are :{grid.best_params_}')"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing model with best paramters\n",
        "dt_2 = DecisionTreeClassifier(max_depth = grid.best_params_['max_depth'],\n",
        "                   min_samples_split = grid.best_params_['min_samples_split'],\n",
        "                   min_samples_leaf = grid.best_params_['min_samples_leaf'],random_state = 42 )"
      ],
      "metadata": {
        "id": "10h9qbuBegi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing with evaluation matrix\n",
        "dt_2_mat = evaluation_matrix(dt_2,X_smote,X_test,y_smote,y_test, X_smote.columns.tolist())\n",
        "scores['Decision Trees tuned'] = dt_2_mat"
      ],
      "metadata": {
        "id": "xLsOaP4kfO4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is a hyperparameter optimization technique that conducts a thorough and systematic search through a predefined parameter grid to identify the optimal hyperparameters for a machine learning model. It is a widely adopted method for hyperparameter tuning due to its simplicity and effectiveness.\n",
        "\n",
        "The selection of a hyperparameter optimization method should take into account several considerations, including the complexity of the parameter space, the available computational resources, and any time constraints. GridSearchCV is particularly suitable when the hyperparameter space is manageable in size and computational resources are not a limiting factor."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "6NJK9zyuil1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following model tuning, noticeable improvements in the model's performance metrics are apparent. The untuned model exhibited signs of overfitting, while the tuned model effectively mitigated overfitting. However, it is worth noting that there was a significant decline in the ROC-AUC score after tuning."
      ],
      "metadata": {
        "id": "MMhebA-IjGSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 Random Forest"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting the model\n",
        "rf = RandomForestClassifier(random_state = 42 )"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "rf_mat = evaluation_matrix(rf,X_smote,X_test,y_smote,y_test,X_smote.columns.tolist())\n",
        "scores['Random Forest'] = rf_mat"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "NGhVeh9-oywn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "param_grid = {'n_estimators' : [10,50,100,200],\n",
        "              'max_depth' : np.arange(10,20),\n",
        "              'min_samples_split': np.arange(2,10)}\n",
        "\n",
        "# Implementing the model\n",
        "rf_2 = RandomForestClassifier(random_state = 42)\n",
        "# Implementing the repeatedstratifiedcv\n",
        "rps = RepeatedStratifiedKFold(n_splits = 4, n_repeats = 5, random_state = 42)\n",
        "\n",
        "# Implementing the gridsearchcv\n",
        "grid = RandomizedSearchCV(rf_2, param_grid, cv = rps, n_iter = 10, n_jobs = -1)\n",
        "\n",
        "grid.fit(X_smote, y_smote)\n",
        "print(f'The best parameters are: {grid.best_params_}')"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model with best parameters\n",
        "rf_best = RandomForestClassifier(n_estimators= grid.best_params_['n_estimators'],\n",
        "                       min_samples_split = grid.best_params_['min_samples_split'],\n",
        "                       max_depth = grid.best_params_['max_depth'], random_state = 42)"
      ],
      "metadata": {
        "id": "58LyURv7Kwi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model with best parameters\n",
        "rf_mat = evaluation_matrix(rf_best,X_smote,X_test,y_smote,y_test,X_smote.columns.tolist())\n",
        "scores['Random Forest tuned'] = rf_mat"
      ],
      "metadata": {
        "id": "omLARVm_BxYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used is RandomizedSearchCV. RandomizedSearchCV is a method that performs a random search over a specified parameter grid to find the best hyperparameters for a model. It is a popular method for hyperparameter tuning because it can be more efficient than exhaustive search methods like GridSearchCV when the parameter space is large.\n",
        "\n",
        "The choice of hyperparameter optimization technique depends on various factors such as the size of the parameter space, the computational resources available, and the time constraints. RandomizedSearchCV can be a good choice when the parameter space is large and computational resources are limited."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "XlrAvfcDLiFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite conducting hyperparameter tuning with randomized values, there was no observed improvement in the model's scores."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 SVM (Support Vector Machines)"
      ],
      "metadata": {
        "id": "VaLtwTcOQc1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impelmenting the model svm\n",
        "sv = SVC(random_state = 42,kernel = 'linear', probability = True)"
      ],
      "metadata": {
        "id": "zpg0NZkUQf-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "uxpn6Ve1Q7X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualozing with evaluation matrix\n",
        "sv_mat = evaluation_matrix(sv,X_smote,X_test,y_smote, y_test, X_smote.columns.tolist())\n",
        "scores['SVM'] = sv_mat"
      ],
      "metadata": {
        "id": "sga_sJCUQ86a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "U8FajDYcR9Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "pS54SOlPRsi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter tuning\n",
        "param_grid = {'C' : np.arange(0.1,10,0.1),\n",
        "              'kernel' : ['rbf','linear','poly','sigmoid'],\n",
        "              'degree' : np.arange(2,6,1)}\n",
        "\n",
        "rsv = RepeatedStratifiedKFold(n_splits = 3, n_repeats = 4, random_state = 42)\n",
        "\n",
        "svc2 = SVC(random_state = 42, probability = True)\n",
        "\n",
        "grid = RandomizedSearchCV(svc2,param_grid, cv = rsv, n_iter = 10)\n",
        "\n",
        "grid.fit(X_smote,y_smote)\n",
        "print(f'The best parameters : {grid.best_params_}')"
      ],
      "metadata": {
        "id": "AKeqrg-6SDSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing model with the best parameters\n",
        "svc = SVC(kernel = grid.best_params_['kernel'],\n",
        "    degree = grid.best_params_['degree'],\n",
        "    C = grid.best_params_['C'], random_state = 42,probability = True)"
      ],
      "metadata": {
        "id": "2GCO78yFaZPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing with evaluation matrix\n",
        "svc_mat = evaluation_matrix(svc,X_smote,X_test,y_smote,y_test,X_smote.columns.tolist())\n",
        "scores['SVM tuned'] = svc_mat"
      ],
      "metadata": {
        "id": "PetaUq1Aa063"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "EBkOsRiQbVFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used is RandomizedSearchCV. RandomizedSearchCV is a method that performs a random search over a specified parameter grid to find the best hyperparameters for a model. It is a popular method for hyperparameter tuning because it can be more efficient than exhaustive search methods like GridSearchCV when the parameter space is large.\n",
        "\n",
        "The choice of hyperparameter optimization technique depends on various factors such as the size of the parameter space, the computational resources available, and the time constraints. RandomizedSearchCV can be a good choice when the parameter space is large and computational resources are limited."
      ],
      "metadata": {
        "id": "KiXpeABkbWuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "9YuJb879bcTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "85A8uRKWbjtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While tuning the SVM model, improvements were evident in the training data scores. However, there was no corresponding improvement in the testing scores, indicating a potential issue of overfitting in the model."
      ],
      "metadata": {
        "id": "cHjJK3tl3rY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5 Xtreme Gradient Boosting"
      ],
      "metadata": {
        "id": "vA1tRV3BiDiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the model\n",
        "xgb = xgb.XGBClassifier()"
      ],
      "metadata": {
        "id": "fGgHTpdk-0Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "j0i0UypO_iXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = evaluation_matrix(xgb,X_smote,X_test,y_smote,y_test,X_smote.columns.tolist())\n",
        "scores['Xtreme Gradient Boosting'] = xgb_model"
      ],
      "metadata": {
        "id": "a29PsV9T_ldE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "fxbJf1FlAsFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4fCAXVPIArC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying hyperparameters to the model\n",
        "param_grid = {'n_estimators' : np.arange(150,200,10),\n",
        "              'max_depth' : np.arange(15,25,1),\n",
        "              'learning_rate' : np.arange(0.02,0.3,0.01)}\n",
        "\n",
        "# Initialize the model\n",
        "xgb_best = XGBClassifier(random_state=0)\n",
        "\n",
        "# Repeat stratified Kfold\n",
        "rspv = RepeatedStratifiedKFold(n_repeats = 5, n_splits = 4, random_state = 42)\n",
        "\n",
        "# Applying Randomized searh Cv\n",
        "grid = RandomizedSearchCV(xgb_best,param_grid, cv = rspv, random_state = 42, n_iter = 10)\n",
        "\n",
        "# fitting the above hyperparameters to the model\n",
        "grid.fit(X_smote,y_smote)\n",
        "\n",
        "# getting the best parameters\n",
        "print(f'The best parameters of the model : {grid.best_params_}')"
      ],
      "metadata": {
        "id": "l4Uxzq7zAtCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the best parameters to the model\n",
        "xgb_best = XGBClassifier(n_estimators = grid.best_params_['n_estimators'],\n",
        "                  max_depth = grid.best_params_['max_depth'],\n",
        "                  learning_rate = grid.best_params_['learning_rate'],\n",
        "                  randon_state = 42)"
      ],
      "metadata": {
        "id": "LAlwRde9CXYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visulaizing the evaluation matrix\n",
        "xgb_mat = evaluation_matrix(xgb_best, X_smote,X_test,y_smote,y_test,X_smote.columns.tolist())\n",
        "scores['Xtreme Gradient Boosting tuned'] = xgb_mat"
      ],
      "metadata": {
        "id": "Mu75_yiCRMQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "aHKKxvGVaZwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used is RandomizedSearchCV. RandomizedSearchCV is a method that performs a random search over a specified parameter grid to find the best hyperparameters for a model. It is a popular method for hyperparameter tuning because it can be more efficient than exhaustive search methods like GridSearchCV when the parameter space is large.\n",
        "\n",
        "The choice of hyperparameter optimization technique depends on various factors such as the size of the parameter space, the computational resources available, and the time constraints. RandomizedSearchCV can be a good choice when the parameter space is large and computational resources are limited."
      ],
      "metadata": {
        "id": "hdw-rGRQaoVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "r_4ccJF0axxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "zTYBnjC2bxbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tuning the model with the best parameters, we can observe that there is no improvement in the model's scores."
      ],
      "metadata": {
        "id": "9WA2wBpAcAkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 6 Naive Bayes"
      ],
      "metadata": {
        "id": "2NApg7wQhOiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the model\n",
        "gb = GaussianNB()"
      ],
      "metadata": {
        "id": "SOVy0O7shJw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "L1HqNHGqhThh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gb_mat = evaluation_matrix(gb,X_smote,X_test,y_smote,y_test,X_smote.columns.tolist())\n",
        "scores['Naive Bayes'] = gb_mat"
      ],
      "metadata": {
        "id": "d5YpWyDihZmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "o6nfGpgtiXXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "rKMIdIPZhZ-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing hyper parameter tuning to the model\n",
        "param_grid = {'var_smoothing' : np.logspace(0,-9,num = 100)}\n",
        "\n",
        "# Initializing the model\n",
        "nb_2 = GaussianNB()\n",
        "\n",
        "# repeated stratifiedkfold\n",
        "rpsv = RepeatedStratifiedKFold(n_repeats = 5, n_splits = 4, random_state = 42)\n",
        "\n",
        "# implementing the randomized search cv\n",
        "grid = RandomizedSearchCV(nb_2, param_grid, cv = rpsv, n_jobs = -1)\n",
        "\n",
        "grid.fit(X_smote,y_smote)\n",
        "\n",
        "# printing the best parameters of the model\n",
        "print(f'The best parameters of the model {grid.best_params_}')"
      ],
      "metadata": {
        "id": "h-vd1qxBhfDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing model with the best parameters\n",
        "naive_bayes = GaussianNB(var_smoothing = grid.best_params_['var_smoothing'])"
      ],
      "metadata": {
        "id": "E_IurQd7qHeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing on the evaluation matrix\n",
        "nb_mat = evaluation_matrix(naive_bayes, X_smote,X_test,y_smote,y_test,X_smote.columns.tolist())\n",
        "scores['Naive Bayes tuned'] = nb_mat"
      ],
      "metadata": {
        "id": "fNH9QKqvqHcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "-1Jxm0cSrvli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used is RandomizedSearchCV. RandomizedSearchCV is a method that performs a random search over a specified parameter grid to find the best hyperparameters for a model. It is a popular method for hyperparameter tuning because it can be more efficient than exhaustive search methods like GridSearchCV when the parameter space is large.\n",
        "\n",
        "The choice of hyperparameter optimization technique depends on various factors such as the size of the parameter space, the computational resources available, and the time constraints. RandomizedSearchCV can be a good choice when the parameter space is large and computational resources are limited."
      ],
      "metadata": {
        "id": "x6vpn-Eer6D9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "qYoo_aunrwYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "LbIM4kbar_WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite tuning the model, there is no discernible improvement in the precision, recall, accuracy, F1 score, or ROC AUC scores of the model."
      ],
      "metadata": {
        "id": "pk29UjeGtdyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores.to_markdown())"
      ],
      "metadata": {
        "id": "--ItZGno3dl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots of Scores"
      ],
      "metadata": {
        "id": "0vVJQVpB5JVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to plot bar chart of the scores\n",
        "def plot_score(x, y,parameter):\n",
        "  \"\"\"This function takes the row indexes from the models list of the respective scores and returns the bar chart of the scores \"\"\"\n",
        "  models = list(scores.columns)\n",
        "  train = scores.iloc[x,:]\n",
        "  test = scores.iloc[y,:]\n",
        "  X = np.arange(len(models))\n",
        "\n",
        "  # Plotting the barchart\n",
        "  plt.figure(figsize = (16,8))\n",
        "  plt.bar(X - 0.2, height = train, width = 0.4, label = 'Train ' + parameter)\n",
        "  plt.bar(X + 0.2, height = test, width = 0.4, label  = 'Test ' + parameter)\n",
        "\n",
        "  plt.xticks(X, models, rotation  = 90)\n",
        "  plt.ylabel(parameter + ' scores')\n",
        "  plt.title(parameter +' scores for every model')\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "apOZnFK9lBLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Precision score"
      ],
      "metadata": {
        "id": "8Len7TvOgWNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the precision scores for every model\n",
        "plot_score(0,1,'precision')   #0,1 are the indexes of the precision train,test in the scores"
      ],
      "metadata": {
        "id": "uta84Fpulj6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Recall score"
      ],
      "metadata": {
        "id": "VUBCBepfgjEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_score(2,3,'recall') #2 ,3 are the indexes of train,test recall scores in the scores"
      ],
      "metadata": {
        "id": "KnYXhjgclyrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### f1_score"
      ],
      "metadata": {
        "id": "3jLMx-PAhtZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_score(4,5,'F1 score') #4, 5 are the indexes of train, test f1_scores in scores"
      ],
      "metadata": {
        "id": "DtxuS0JAmJEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Accuracy score\n"
      ],
      "metadata": {
        "id": "0-xs_FQ4isJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_score(6,7,'Accuracy')  # 6, 7 are the indexes of train, test accuracy scores in the scores"
      ],
      "metadata": {
        "id": "kiDI95jAjtS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ROC AUC score"
      ],
      "metadata": {
        "id": "My7z0ShRnFjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_score(8,9,'ROC_AUC') # 8, 9 are the indexes of the train, test roc auc scores in the scores"
      ],
      "metadata": {
        "id": "ppA96OBOkkki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Selection of the best model"
      ],
      "metadata": {
        "id": "I_RN2qsFou18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "FGcqDtGXnY3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we will consider those models whose scores are lesser than 0.95 and will remove those models which are overfitted.\n",
        "\n"
      ],
      "metadata": {
        "id": "jR4ddPJAo5A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing those models whose scores are greater than 0.95\n",
        "scores_t = scores.T   # Taking a transpose of the column\n",
        "model_names = scores_t.index\n",
        "lst = []\n",
        "for i,value in enumerate(scores_t.values):\n",
        "  if (value >= 0.95).any():                    #condition of the scores\n",
        "    lst.append((value,model_names[i]))\n",
        "\n",
        "print(lst)"
      ],
      "metadata": {
        "id": "R50pxM1to2Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assigning new df of scores after filtering the scores\n",
        "scores_final = scores_t.drop(pd.DataFrame(lst)[1], axis = 0)\n",
        "scores_final"
      ],
      "metadata": {
        "id": "amh_nR4mrtxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function to get the best\n",
        "def best_model(df, metrics):\n",
        "  best_models = {}\n",
        "  for metric in metrics:\n",
        "    best_score = df[metric +'_test'].max()\n",
        "    model_name  = df[df[metric + '_test'] == best_score].index[0]\n",
        "    best_models[metric] = model_name\n",
        "  return best_models"
      ],
      "metadata": {
        "id": "e6_bbstZxKPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = ['Precision','Recall','F1_score','Accuracy','ROC_AUC']\n",
        "\n",
        "final_models = best_model(scores_final, metrics)\n",
        "\n",
        "for metric,best_model in final_models.items():\n",
        "  print(f\"{metric} - {best_model} - {scores_final[metric+'_test'][best_model].round(4)}\")\n"
      ],
      "metadata": {
        "id": "LLXyRogwznyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "5J8H9-7y2_7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After a thorough assessment of the possible impacts of false positives and false negatives within the scope of our business objectives, I have made the deliberate choice to prioritize the \"recall\" metric as the primary evaluation criterion for our CHD risk prediction model. In essence, our key aim is to maximize the identification of true positives, accurately recognizing individuals with CHD risk, while concurrently minimizing the occurrence of false negatives—instances where patients with actual CHD risk might be incorrectly classified as not at risk. This approach ensures that our primary focus is on capturing as many patients with CHD risk as possible, even if it results in the inclusion of some false positives in the process."
      ],
      "metadata": {
        "id": "bzeRxlfQ3GFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "yk_EKaw93F_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After a comprehensive evaluation of various machine learning models using the Framingham Heart Study dataset, our final selection for the prediction model is logistic regression. This decision was driven by the model's outstanding performance in terms of recall, which assesses its ability to accurately identify patients at risk of Coronary Heart Disease (CHD). In our assessment, logistic regression outperformed the other models, yielding the highest recall score.\n",
        "\n",
        "Our choice to prioritize recall as the primary evaluation metric stems from its crucial role in fulfilling our business objectives. Correctly identifying individuals with CHD risk is paramount, and high recall aids in achieving this goal. We emphasize capturing as many patients at risk as possible, even if it leads to some false positives. In summary, we are confident that logistic regression is the optimal choice to support our specific requirements and contribute positively to our business outcomes."
      ],
      "metadata": {
        "id": "g1DgFvyy3Kns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}